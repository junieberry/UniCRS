{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tako/anaconda3/envs/crs/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import pickle as pkl\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21492it [00:00, 37647.70it/s]\n",
      "2351it [00:00, 36895.66it/s]\n",
      "2788it [00:00, 37899.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# all entity: 4206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_item_set(file):\n",
    "    entity = set()\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f):\n",
    "            line = json.loads(line)\n",
    "            for message in line['dialog_history']:\n",
    "                for e in message['movie']:\n",
    "                    entity.add(e)\n",
    "    return entity\n",
    "\n",
    "all_entity = set()\n",
    "file_list = [\n",
    "    'processed_train.jsonl',\n",
    "    'processed_valid.jsonl',\n",
    "    'processed_test.jsonl'\n",
    "]\n",
    "\n",
    "for file in file_list:\n",
    "    all_entity |= get_item_set(file)\n",
    "\n",
    "print(f'# all entity: {len(all_entity)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kg(path):\n",
    "    print('load kg')\n",
    "    kg = defaultdict(list)  # {head entity: [(relation, tail entity)]}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f):\n",
    "            tuples = line.strip().split()\n",
    "            if tuples and len(tuples) == 4 and tuples[-1] == \".\":\n",
    "                h, r, t = tuples[:3]\n",
    "                kg[h].append((r, t))\n",
    "    return kg\n",
    "\n",
    "with open('../dbpedia/kg.pkl', 'rb') as f:\n",
    "    kg = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.defaultdict"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract subkg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4206 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4206/4206 [00:00<00:00, 63624.09it/s]\n",
      "100%|██████████| 17669/17669 [00:00<00:00, 143200.57it/s]\n",
      "100%|██████████| 17833/17833 [00:00<00:00, 231984.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# entity: 22660\n",
      "# relation: 25\n"
     ]
    }
   ],
   "source": [
    "def extract_subkg(kg, seed_set, n_hop):\n",
    "    \"\"\"extract subkg from seed_set by n_hop\n",
    "\n",
    "    Args:\n",
    "        kg (dict): {head entity: [(relation, tail entity)]}\n",
    "        seed_set (list or set): [entity]\n",
    "        n_hop (int):\n",
    "\n",
    "    Returns:\n",
    "        subkg (dict): {head entity: [(relation, tail entity)]}, extended by n_hop\n",
    "    \"\"\"\n",
    "    print('extract subkg')\n",
    "\n",
    "    subkg = defaultdict(list)  # {head entity: [(relation, tail entity)]}\n",
    "    subkg_hrt = set()  # {(head_entity, relation, tail_entity)}\n",
    "\n",
    "    ripple_set = None\n",
    "    for hop in range(n_hop):\n",
    "        memories_h = set()  # [head_entity]\n",
    "        memories_r = set()  # [relation]\n",
    "        memories_t = set()  # [tail_entity]\n",
    "\n",
    "        if hop == 0:\n",
    "            tails_of_last_hop = seed_set  # [entity]\n",
    "        else:\n",
    "            tails_of_last_hop = ripple_set[2]  # [tail_entity]\n",
    "\n",
    "        for entity in tqdm(tails_of_last_hop):\n",
    "            for relation_and_tail in kg[entity]:\n",
    "                h, r, t = entity, relation_and_tail[0], relation_and_tail[1]\n",
    "                if (h, r, t) not in subkg_hrt:\n",
    "                    subkg_hrt.add((h, r, t))\n",
    "                    subkg[h].append((r, t))\n",
    "                memories_h.add(h)\n",
    "                memories_r.add(r)\n",
    "                memories_t.add(t)\n",
    "\n",
    "        ripple_set = (memories_h, memories_r, memories_t)\n",
    "\n",
    "    return subkg\n",
    "\n",
    "\n",
    "def kg2id(kg):\n",
    "    entity_set = all_entity\n",
    "    with open('relation_set.json', encoding='utf-8') as f:\n",
    "        relation_set = json.load(f)\n",
    "\n",
    "    for head, relation_tails in tqdm(kg.items()):\n",
    "        for relation_tail in relation_tails:\n",
    "            if relation_tail[0] in relation_set:\n",
    "                entity_set.add(head)\n",
    "                entity_set.add(relation_tail[1])\n",
    "\n",
    "    entity2id = {e: i for i, e in enumerate(entity_set)}\n",
    "    print(f\"# entity: {len(entity2id)}\")\n",
    "    relation2id = {r: i for i, r in enumerate(relation_set)}\n",
    "    relation2id['self_loop'] = len(relation2id)\n",
    "    print(f\"# relation: {len(relation2id)}\")\n",
    "\n",
    "    kg_idx = {}\n",
    "    for head, relation_tails in kg.items():\n",
    "        if head in entity2id:\n",
    "            head = entity2id[head]\n",
    "            kg_idx[head] = [(relation2id['self_loop'], head)]\n",
    "            for relation_tail in relation_tails:\n",
    "                if relation_tail[0] in relation2id and relation_tail[1] in entity2id:\n",
    "                    kg_idx[head].append((relation2id[relation_tail[0]], entity2id[relation_tail[1]]))\n",
    "\n",
    "    return entity2id, relation2id, kg_idx\n",
    "\n",
    "subkg = extract_subkg(kg, all_entity, 2)\n",
    "entity2id, relation2id, subkg = kg2id(subkg)\n",
    "\n",
    "with open('dbpedia_subkg.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(subkg, f, ensure_ascii=False)\n",
    "with open('entity2id.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(entity2id, f, ensure_ascii=False)\n",
    "with open('relation2id.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(relation2id, f, ensure_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
